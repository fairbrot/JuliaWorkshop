{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse least squares regression\n",
    "\n",
    "Sparse least squares regression is a method to construct robust regression models. We consider the simple model on slides 13-14 in the document optim_software.pdf.\n",
    "\n",
    "In order to solve this problem you will need a solver which supports mixed integer second-order cone programs, for example Gurobi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1) Using JuMP, implement a function which solves the sparse least-squares regression problem on slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve_sparse_least_squares (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Description\n",
    "Solves sparse least-squares problem.\n",
    "\n",
    "min_β || y - Xβ ||^2\n",
    "s.t.\n",
    "||β||_0 ⫹ p\n",
    "\n",
    "# Arguments\n",
    "`X::Matrix{Float64}`: m x n design matrix\n",
    "`y::Vector{Float64}`: output observations\n",
    "`p::Int`: Maximum number of non-negative coeffients\n",
    "\n",
    "# Returns\n",
    "`(β::Vector{Float64}, res::Vector{Float64})`: model coefficients and residuals vectors\n",
    "\"\"\"\n",
    "function solve_sparse_least_squares(X::Matrix{Float64}, y::Vector{Float64}, p::Int)\n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function can be used to generate `X` and `y` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5159698655771929,0.6446383677396061,0.0,0.0,0.50634864285824,0.0,0.6387518709907682,0.0,0.9868765575572167,0.0],\n",
       "20x10 Array{Float64,2}:\n",
       " 0.394005  0.730068  0.263608   0.925259  …  0.67043    0.722595  0.320934\n",
       " 0.497115  0.429411  0.327182   0.152297     0.186435   0.791346  0.465278\n",
       " 0.353522  0.979     0.602395   0.479236     0.786573   0.46822   0.257853\n",
       " 0.844937  0.498688  0.371709   0.916853     0.841595   0.724236  0.562422\n",
       " 0.641569  0.419138  0.147911   0.343401     0.0244967  0.886502  0.961851\n",
       " 0.460836  0.377406  0.0101558  0.535995  …  0.800563   0.380198  0.552721\n",
       " 0.513601  0.316604  0.921863   0.174299     0.343503   0.543394  0.514913\n",
       " 0.645096  0.612702  0.581198   0.720187     0.99819    0.193141  0.687568\n",
       " 0.296647  0.559966  0.521633   0.472408     0.3275     0.174133  0.447183\n",
       " 0.843367  0.405142  0.489118   0.392661     0.271633   0.291915  0.420626\n",
       " 0.990082  0.856129  0.224498   0.810399  …  0.43609    0.204859  0.853053\n",
       " 0.701745  0.266796  0.106247   0.903366     0.724455   0.217524  0.646472\n",
       " 0.730252  0.163823  0.873316   0.942459     0.359819   0.171348  0.843829\n",
       " 0.698911  0.653499  0.403112   0.85427      0.453046   0.98503   0.959379\n",
       " 0.493463  0.478347  0.690841   0.521592     0.56513    0.62152   0.366808\n",
       " 0.193299  0.140706  0.968536   0.493326  …  0.55126    0.530651  0.970887\n",
       " 0.155474  0.196787  0.834671   0.646662     0.6992     0.735091  0.346134\n",
       " 0.755156  0.556682  0.0339499  0.167069     0.743632   0.650411  0.618118\n",
       " 0.907849  0.508808  0.774332   0.483601     0.0324562  0.593571  0.275536\n",
       " 0.644512  0.900756  0.22888    0.498239     0.789063   0.830314  0.86896 ,\n",
       "\n",
       "[2.266895622564485,2.1681528846635527,2.524335223120454,2.269542199848177,2.8659574413820077,1.7302208114336026,2.3747193946780727,1.8560355251595433,1.0653750328311893,1.5710337903676268,1.8136263049981367,1.230116774464452,1.6393718580480325,2.596132009383074,1.748732555640154,1.897244733386478,1.312765788533886,1.6574899673076902,2.165293426195321,2.664815811900782])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_problem(p::Int, n::Int, m::Int)\n",
    "    raw_β = rand(n)\n",
    "    sort_β = sort(raw_β)\n",
    "    ord_p = sort_β[n-p]\n",
    "    β = map(x-> (x>ord_p? x:0.0), raw_β)\n",
    "    X = rand(m,n)\n",
    "    y = X*β + 0.5*rand(m)\n",
    "    return β, X, y\n",
    "end\n",
    "\n",
    "p, n, m = 5, 10, 20\n",
    "β, X, y = generate_problem(p,n,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-starts\n",
    "\n",
    "For large problems, it may help to solve the problem with a `warm-start`, providing your model with a good initial solution to speed up the optimization. In this case, a good solution may be to solve the standard least-squares problem and rounding the down to zero the $(n - p)$ coefficients of $\\beta$ with the smallest  (absolute) size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Modify your function to use a warm start. You can use the `llsq` function in the `MultivariateStats` package to solve the standard least squares problem. See http://multivariatestatsjl.readthedocs.io/en/latest/lreg.html for documentation on this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: warm-start may only make a positive difference for larger problem sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
